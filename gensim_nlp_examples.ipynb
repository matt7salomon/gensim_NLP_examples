{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b01607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary: {'and': 0, 'are': 1, 'fun': 2, 'language': 3, 'learning': 4, 'machine': 5, 'natural': 6, 'processing': 7, 'favorite': 8, 'is': 9, 'my': 10, 'programming': 11, 'python': 12, 'about': 13, 'enjoy': 14, 'i': 15, 'reading': 16}\n",
      "Corpus: [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(3, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(4, 1), (5, 1), (13, 1), (14, 1), (15, 1), (16, 1)]]\n"
     ]
    }
   ],
   "source": [
    "### 1. Creating a Gensim Dictionary and Corpus\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Natural language processing and machine learning are fun\",\n",
    "    \"Python is my favorite programming language\",\n",
    "    \"I enjoy reading about machine learning\"\n",
    "]\n",
    "\n",
    "# Tokenize the documents\n",
    "texts = [doc.lower().split() for doc in documents]\n",
    "\n",
    "# Create a dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Create a corpus (Bag of Words)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Print dictionary and corpus\n",
    "print(\"Dictionary:\", dictionary.token2id)\n",
    "print(\"Corpus:\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398591b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.111*\"machine\" + 0.111*\"learning\" + 0.067*\"fun\"')\n",
      "(1, '0.104*\"language\" + 0.103*\"programming\" + 0.103*\"is\"')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### 2. Training a Topic Model (LDA)\n",
    "\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Using the previously created corpus and dictionary\n",
    "# Train an LDA model (Latent Dirichlet Allocation)\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=3)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d81f0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine': [-0.01631559  0.00899109 -0.00827429  0.00164873  0.01699636 -0.00892452\n",
      "  0.00903518 -0.01357433 -0.00709761  0.01879663 -0.00315458  0.0006425\n",
      " -0.0082806  -0.01536514 -0.00301648  0.0049403  -0.00177546  0.01106799\n",
      " -0.00548653  0.00451997  0.01091197  0.01669232 -0.00290673 -0.01841601\n",
      "  0.00874086  0.00114438  0.01488423 -0.00162644 -0.00527695 -0.01750608\n",
      " -0.00171279  0.00565267  0.01080237  0.01410518 -0.01140631  0.00371709\n",
      "  0.0121772  -0.00959642 -0.0062141   0.01359509  0.00326311  0.00038021\n",
      "  0.00694774  0.00043512  0.01923844  0.01012189 -0.01783467 -0.0140836\n",
      "  0.00180344  0.01278435]\n",
      "Most similar words to 'language': [('I', 0.21057182550430298), ('my', 0.1670549064874649), ('programming', 0.15019884705543518), ('are', 0.13204392790794373), ('learning', 0.1267007291316986), ('reading', 0.0998455360531807), ('natural', 0.059367649257183075), ('machine', 0.0423765629529953), ('favorite', 0.04066995531320572), ('processing', 0.012373912148177624)]\n"
     ]
    }
   ],
   "source": [
    "### 3. Word2Vec Model for Word Embeddings\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"natural language processing and machine learning are fun\".split(),\n",
    "    \"python is my favorite programming language\".split(),\n",
    "    \"I enjoy reading about machine learning\".split()\n",
    "]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=2)\n",
    "\n",
    "# Get the vector for a word\n",
    "vector = word2vec_model.wv['machine']\n",
    "print(\"Vector for 'machine':\", vector)\n",
    "\n",
    "# Find the most similar words to 'language'\n",
    "similar_words = word2vec_model.wv.most_similar('language')\n",
    "\n",
    "print(\"Most similar words to 'language':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac75fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.42998768831312806), (1, 0.42998768831312806), (2, 0.42998768831312806), (3, 0.1586956620869655), (4, 0.1586956620869655), (5, 0.1586956620869655), (6, 0.42998768831312806), (7, 0.42998768831312806)]\n",
      "[(3, 0.16284991207632715), (8, 0.44124367556640004), (9, 0.44124367556640004), (10, 0.44124367556640004), (11, 0.44124367556640004), (12, 0.44124367556640004)]\n",
      "[(4, 0.17855490118826328), (5, 0.17855490118826328), (13, 0.48379652089574265), (14, 0.48379652089574265), (15, 0.48379652089574265), (16, 0.48379652089574265)]\n"
     ]
    }
   ],
   "source": [
    "### 4. TF-IDF Model\n",
    "\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Create a TF-IDF model from the corpus\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "# Transform a document using the TF-IDF model\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "# Print the TF-IDF values for the first document\n",
    "for doc in tfidf_corpus:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38197ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for document 0: [-0.02617824 -0.02989686 -0.04943514  0.04278     0.01787857  0.00126838\n",
      " -0.04944359 -0.02587047 -0.04864132  0.01000107  0.01413195  0.02325049\n",
      " -0.0215772  -0.0158158  -0.01545884 -0.04362763  0.01084296  0.04613708\n",
      " -0.04757599 -0.01728128]\n",
      "Most similar documents to document 0: [('1', 0.26511624455451965), ('2', -0.13240714371204376), ('3', -0.3683653473854065)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### 5. Doc2Vec Model\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love playing football\",\n",
    "    \"Football is a great sport\",\n",
    "    \"I play football every weekend\",\n",
    "    \"I love watching football matches\"\n",
    "]\n",
    "\n",
    "# Prepare the training data for Doc2Vec\n",
    "tagged_data = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "\n",
    "# Train a Doc2Vec model\n",
    "doc2vec_model = Doc2Vec(tagged_data, vector_size=20, window=2, min_count=1, workers=4)\n",
    "\n",
    "# Get the vector for a specific document (e.g., document 0)\n",
    "doc_vector = doc2vec_model.dv['0']\n",
    "print(\"Vector for document 0:\", doc_vector)\n",
    "\n",
    "# Find most similar documents to the first one\n",
    "similar_docs = doc2vec_model.dv.most_similar('0')\n",
    "print(\"Most similar documents to document 0:\", similar_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8dd8598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.431*\"machine\" + 0.431*\"learning\" + 0.352*\"language\"')\n",
      "(1, '0.365*\"language\" + 0.365*\"my\" + 0.365*\"programming\"')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### 6. Applying LSI (Latent Semantic Indexing)\n",
    "\n",
    "\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "# Train an LSI model using the corpus\n",
    "lsi_model = LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "\n",
    "# Print the topics\n",
    "lsi_topics = lsi_model.print_topics(num_words=3)\n",
    "for topic in lsi_topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19cbb223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 0.14832774), (2, 0.99989796)]\n"
     ]
    }
   ],
   "source": [
    "### 7. Finding Similar Documents with Similarity Queries\n",
    "\n",
    "\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "# Build a similarity index using the LDA model\n",
    "index = MatrixSimilarity(lda_model[corpus])\n",
    "\n",
    "# Query the similarity of the first document against the corpus\n",
    "query_bow = corpus[0]\n",
    "query_lda = lda_model[query_bow]\n",
    "sims = index[query_lda]\n",
    "\n",
    "# Print similarity scores\n",
    "print(list(enumerate(sims)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1de8d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NLU using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49e26bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mattsalomon/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mattsalomon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary: {'artificial': 0, 'field': 1, 'intelligence': 2, 'language': 3, 'natural': 4, 'nlp': 5, 'processing': 6, 'accuracy': 7, 'deep': 8, 'greatly': 9, 'improved': 10, 'learning': 11, 'methods': 12, 'models': 13, 'algorithms': 14, 'building': 15, 'essential': 16, 'machine': 17, 'systems': 18, 'analysis': 19, 'gensim': 20, 'library': 21, 'modeling': 22, 'popular': 23, 'text': 24, 'topic': 25, 'industries': 26, 'transforming': 27, 'various': 28}\n",
      "Corpus: [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(3, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(5, 1), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)], [(19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(0, 1), (2, 1), (11, 1), (17, 1), (26, 1), (27, 1), (28, 1)]]\n",
      "\n",
      "Topics found by the LDA Model:\n",
      "(0, '0.069*\"popular\" + 0.069*\"analysis\" + 0.069*\"topic\" + 0.069*\"gensim\"')\n",
      "(1, '0.081*\"learning\" + 0.058*\"machine\" + 0.058*\"artificial\" + 0.057*\"nlp\"')\n",
      "\n",
      "Topic Distribution for the New Document:\n",
      "Topic 0: 0.088\n",
      "Topic 1: 0.912\n",
      "\n",
      "Coherence Score: 0.3295\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text documents (corpus)\n",
    "documents = [\n",
    "    \"Natural language processing (NLP) is a field of artificial intelligence.\",\n",
    "    \"Deep learning methods have greatly improved the accuracy of language models.\",\n",
    "    \"Machine learning algorithms are essential in building NLP systems.\",\n",
    "    \"Gensim is a popular library for topic modeling and text analysis.\",\n",
    "    \"Artificial intelligence and machine learning are transforming various industries.\"\n",
    "]\n",
    "\n",
    "### Step 1: Text Preprocessing\n",
    "\n",
    "# Tokenize the documents and remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "    return [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Preprocess all documents\n",
    "texts = [preprocess(doc) for doc in documents]\n",
    "\n",
    "### Step 2: Create Gensim Dictionary and Corpus\n",
    "\n",
    "# Create a dictionary from the texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Create a bag-of-words representation of the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Print the dictionary and corpus\n",
    "print(\"Dictionary:\", dictionary.token2id)\n",
    "print(\"Corpus:\", corpus)\n",
    "\n",
    "### Step 3: Train the LDA Model\n",
    "\n",
    "# Train an LDA model with 2 topics\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=10)\n",
    "\n",
    "# Print the topics\n",
    "print(\"\\nTopics found by the LDA Model:\")\n",
    "topics = lda_model.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "### Step 4: Understanding New Documents\n",
    "\n",
    "# New unseen document\n",
    "new_doc = \"Artificial intelligence and deep learning are key technologies in NLP.\"\n",
    "\n",
    "# Preprocess the new document\n",
    "new_bow = dictionary.doc2bow(preprocess(new_doc))\n",
    "\n",
    "# Get the topic distribution for the new document\n",
    "new_doc_topics = lda_model.get_document_topics(new_bow)\n",
    "\n",
    "# Print the topic distribution for the new document\n",
    "print(\"\\nTopic Distribution for the New Document:\")\n",
    "for topic, prob in new_doc_topics:\n",
    "    print(f\"Topic {topic}: {prob:.3f}\")\n",
    "\n",
    "### Step 5: Coherence Score for Model Evaluation\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute the coherence score for the model\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(f\"\\nCoherence Score: {coherence_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b9d6981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mattsalomon/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b08df831",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip /Users/mattsalomon/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98114cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mattsalomon/gensim-data/word2vec-google-news-300/__init__.py\r\n",
      "/Users/mattsalomon/gensim-data/word2vec-google-news-300/word2vec-google-news-300\r\n",
      "\r\n",
      "/Users/mattsalomon/gensim-data/word2vec-google-news-300/__pycache__:\r\n",
      "__init__.cpython-310.pyc\r\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/mattsalomon/gensim-data/word2vec-google-news-300/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7e06a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.05126953 -0.02233887 -0.17285156  0.16113281 -0.08447266  0.05737305\n",
      "  0.05859375 -0.08251953 -0.01538086 -0.06347656]\n",
      "(300,)\n",
      "[('queen', 0.7118192911148071), ('monarch', 0.6189674735069275), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411403656006)]\n",
      "cereal\n",
      "0.76640123\n"
     ]
    }
   ],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('/Users/mattsalomon/gensim-data/word2vec-google-news-300/word2vec-google-news-300',binary=True)\n",
    "\n",
    "dog = model['dog']\n",
    "print(dog.shape)\n",
    "print(dog[:10])\n",
    "\n",
    "# Deal with an out of dictionary word: Özgür\n",
    "if 'Özgür' in model:\n",
    "    print(model['Özgür'].shape)\n",
    "else:\n",
    "    print('{0} is an out of dictionary word'.format('Özgür'))\n",
    "\n",
    "# Some predefined functions that show content related information for given words\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "\n",
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "\n",
    "print(model.similarity('woman', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ee095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
